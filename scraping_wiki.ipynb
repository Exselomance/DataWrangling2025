{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGj_iEJWeJDj",
        "outputId": "574c92e2-6fec-4deb-d472-be0ef2125625"
      },
      "outputs": [],
      "source": [
        "%pip install scrapy\n",
        "%pip install \"pymongo[srv]\"\n",
        "%pip install python-dotenv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Script Overview\n",
        "\n",
        "The script consists of:\n",
        "\n",
        "1. **WikiSpider Class**: A Scrapy spider that:\n",
        "   - Crawls the Wikipedia page for highest-grossing films.\n",
        "   - Extracts movie titles, release years, box office revenue, and country of origin.\n",
        "   - Follows links to individual movie pages for more details.\n",
        "   - Stores the data in a MongoDB collection.\n",
        "\n",
        "2. **MongoDB Connection**:\n",
        "   - Uses `pymongo` to connect to a MongoDB database.\n",
        "   - Stores scraped movie data in the database under the collection.\n",
        "\n",
        "3. **Crawler Process**:\n",
        "   - Initializes and starts the Scrapy spider."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jG_wG6nLerTg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import scrapy\n",
        "\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "from pymongo.mongo_client import MongoClient\n",
        "from pymongo.server_api import ServerApi\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "\n",
        "load_dotenv()\n",
        "class WikiSpider(scrapy.Spider):\n",
        "    name = \"high_rates_films\"\n",
        "    start_urls = [\"https://en.wikipedia.org/wiki/List_of_highest-grossing_films\"]\n",
        "    def __init__(self):\n",
        "        uri = os.getenv(\"MONGODB_URI\")\n",
        "        db_name = os.getenv(\"MONGODB_DB_NAME\")\n",
        "        collection_name = os.getenv(\"MONGODB_COLLECTION\")\n",
        "        self.client = MongoClient(uri, server_api=ServerApi('1'))\n",
        "        self.db = self.client[db_name]\n",
        "        self.collection = self.db[collection_name]\n",
        "    def parse(self, response):\n",
        "        rows = response.xpath('//*[@id=\"mw-content-text\"]/div[1]/table[1]/tbody')\n",
        "        titles = set()\n",
        "        for row in rows.xpath('.//tr'):\n",
        "            if row.xpath('.//th/span//text()').get() is not None:\n",
        "                title = row.xpath('.//th/span//text()').get()\n",
        "                link = row.xpath('.//th/span/i/a/@href').get()\n",
        "                year = row.xpath(\".//td[4]//text()\").get()\n",
        "                year = row.xpath(\".//td[4]//text()\").get()\n",
        "                if year:\n",
        "                    year = year.strip()\n",
        "\n",
        "        \n",
        "                gross_value = row.xpath(\".//td[3]//text()\").get()\n",
        "                if gross_value:\n",
        "                    gross_value = row.xpath(\".//td[3]/text()[not(parent::sup)]\").getall()[-1].replace('$', '').replace(',', '').strip()\n",
        "            else:\n",
        "                title = row.xpath('.//th/i/a//text()').get()\n",
        "                link = row.xpath('.//th/i/a/@href').get()\n",
        "                year = row.xpath(\".//td[4]//text()\").get()\n",
        "                year = row.xpath(\".//td[4]//text()\").get()\n",
        "                if year:\n",
        "                    year = year.strip()\n",
        "\n",
        "        \n",
        "                gross_value = row.xpath(\".//td[3]//text()\").get()\n",
        "                if gross_value:\n",
        "                    gross_value = row.xpath(\".//td[3]/text()[not(parent::sup)]\").getall()[-1].replace('$', '').replace(',', '').strip()\n",
        "            if (title and link) and (title not in titles):\n",
        "                    titles.add(title)\n",
        "                    full_link = response.urljoin(link)    # Передаем название фильма и ссылку в следующий запрос через meta\n",
        "                    yield scrapy.Request(\n",
        "                        url=full_link, \n",
        "                        callback=self.parse_movie, \n",
        "                        meta={\"title\": title, \"link\": full_link, \"year\" : year, \"gross_value\" : gross_value}\n",
        "            )\n",
        "\n",
        "    def parse_movie(self, response):\n",
        "        title = response.meta[\"title\"].replace(\"\\u2013\", \"-\")\n",
        "        link = response.meta[\"link\"]\n",
        "        year = response.meta[\"year\"]\n",
        "        gross_value = response.meta[\"gross_value\"]\n",
        "        movie_info = response.xpath('//*[@id=\"mw-content-text\"]/div[1]/table[1]/@class')\n",
        "        if movie_info.get() == \"box-Expand_language plainlinks metadata ambox ambox-notice skin-invert-image\":\n",
        "             movie_info = response.xpath('//*[@id=\"mw-content-text\"]/div[1]/table[2]')\n",
        "        else:\n",
        "            movie_info = response.xpath('//*[@id=\"mw-content-text\"]/div[1]/table[1]')\n",
        "        countries = None\n",
        "        directed_by = None\n",
        "        for row in movie_info.xpath('.//tr'):\n",
        "            header = row.xpath('.//th/text()').get()\n",
        "            if header:\n",
        "                if header.strip() == \"Directed by\":\n",
        "                    directed_by = row.xpath('.//td//li/text()').getall()\n",
        "                if not directed_by:\n",
        "                    directed_by = row.xpath('.//td//a/text()').getall()\n",
        "                elif header.strip() == \"Country\":\n",
        "                    countries = row.xpath('.//td//text()').getall()\n",
        "                elif header.strip() == \"Countries\":\n",
        "                    countries = row.xpath('.//td//li/text()').getall()\n",
        "                    if not countries:\n",
        "                        countries = row.xpath('.//td//text()').getall()\n",
        "                        countries = [value.strip() for value in countries if value.strip()]\n",
        "\n",
        "        movie_doc = {\n",
        "            \"Film Title\": title,\n",
        "            \"Release Year\": year,\n",
        "            \"Director(s)\": directed_by,\n",
        "            \"Box Office Revenue\": gross_value,\n",
        "            \"Country of Origin\": countries\n",
        "        }\n",
        "\n",
        "        self.collection.insert_one(movie_doc)\n",
        "        yield movie_doc\n",
        "process = CrawlerProcess(settings=None)\n",
        "\n",
        "process.crawl(WikiSpider)\n",
        "\n",
        "\n",
        "process.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exporting Scraped Data to JSON\n",
        "\n",
        "This script retrieves movie data from a MongoDB database and saves it as a JSON file for use in a frontend application.\n",
        "\n",
        "### Steps:\n",
        "\n",
        "1. **MongoDB Connection**:\n",
        "   - Connects to a MongoDB database using `pymongo`.\n",
        "   - Retrieves all documents from the `high_grossing_films` collection in the `movies_db` database.\n",
        "\n",
        "2. **Data Retrieval**:\n",
        "   - Queries the database for all movie records while excluding the `_id` field.\n",
        "\n",
        "3. **Saving to JSON**:\n",
        "   - Writes the retrieved data to `output.json` inside the `frontend/public/` directory.\n",
        "   - Ensures proper encoding and formatting for easy use in frontend applications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "from pymongo import MongoClient\n",
        "from pymongo.server_api import ServerApi\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "\n",
        "uri = os.getenv(\"MONGODB_URI\")\n",
        "db_name = os.getenv(\"MONGODB_DB_NAME\")\n",
        "collection_name = os.getenv(\"MONGODB_COLLECTION\")\n",
        "\n",
        "\n",
        "client = MongoClient(uri, server_api=ServerApi('1'))\n",
        "db = client[db_name]\n",
        "collection = db[collection_name]\n",
        "\n",
        "data = list(collection.find({}, {\"_id\": 0}))\n",
        "\n",
        "\n",
        "output_path = os.path.join(\"frontend\", \"public\", \"output.json\")\n",
        "\n",
        "\n",
        "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(data, f, ensure_ascii=False, indent=4)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
